[program:ollama-server]
; Ollama server supervisor configuration
; This runs the Ollama server daemon which serves models via HTTP API
; Models are loaded on-demand when requested via the API (not via environment variables)
command=ollama serve
directory=/usr/local/bin
autostart=true
autorestart=true
startsecs=10
startretries=3
user=root
; OLLAMA_HOST sets the server to listen on all interfaces (0.0.0.0) on port 11434
; Note: OLLAMA_MODEL is NOT a valid environment variable - models load on-demand via API
; Only OLLAMA_HOST is needed here
environment=OLLAMA_HOST="0.0.0.0:11434"
redirect_stderr=true
stdout_logfile=/var/log/supervisor/ollama-server.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/ollama-server-error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
stopsignal=TERM
stopwaitsecs=30
killasgroup=true
priority=999



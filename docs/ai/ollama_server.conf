[program:ollama-server]
; Ollama server supervisor configuration
; This runs the Ollama server daemon which serves models via HTTP API
; Models are loaded on-demand when requested via the API
command=ollama serve
directory=/usr/local/bin
autostart=true
autorestart=true
startsecs=10
startretries=3
user=root
; OLLAMA_HOST sets the server to listen on all interfaces (0.0.0.0) on port 11434
; OLLAMA_MODEL is included for reference (models load on-demand via API)
environment=OLLAMA_HOST="0.0.0.0:11434",OLLAMA_MODEL="llama3.1:8b"
redirect_stderr=true
stdout_logfile=/var/log/supervisor/ollama-server.log
stdout_logfile_maxbytes=50MB
stdout_logfile_backups=10
stderr_logfile=/var/log/supervisor/ollama-server-error.log
stderr_logfile_maxbytes=50MB
stderr_logfile_backups=10
stopsignal=TERM
stopwaitsecs=30
killasgroup=true
priority=999


